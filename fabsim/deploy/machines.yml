default:
  modules:
    all: []
    dummy: []
  nb_process: 1
  venv: false
  module_load_at_connect: true
  cores: 4
  run_prefix_commands: []
  home_path_template: /home/$username
  fabric_dir: FabSim
  runtime_path_template: $home_path
  corespernode: 1
  cpuspertask: 1
  nodes: 1
  cores_per_replica: '${cores}'
  job_dispatch: ''
  remote_path_template: $home_path/$fabric_dir
  work_path_template: $runtime_path/$fabric_dir
  batch_header: no_batch
  run_command: mpirun -np $cores
  temp_path_template: /tmp
  job_desc: ''
  job_name_template: '${config}_${machine_name}_${cores}${job_desc}'
  manual_ssh: false
  manual_gsissh: false
  manual_sshpass: false
  ssh_monsoon_mode: false
  local_templates_path:
    - $fabsim_root/deploy/templates
  stat: squeue -u $username
  cancel_job_command: qdel
  local_config_file_path:
    - $fabsim_root/config_files
  python3_command: python3
  prevent_results_overwrite: ignore
  rich_console: true
  dry_run: false
localhost:
  remote: localhost
  python_build: lib/python2.7
  home_path_template: /home/$username
  manual_ssh: true
  batch_header: bash_header
cartesius:
  max_job_name_chars: 15
  job_dispatch: sbatch
  stat: squeue -u $username
  run_command: mpiexec -n $cores
  batch_header: slurm-cartesius
  remote: cartesius.surfsara.nl
  home_path_template: /home/$username
  runtime_path_template: /scratch-local/$username
  modules:
    all: []
    gromacs:
      - load gromacs
  temp_path_template: $work_path/tmp
  queue: normal
  python_build: python2.7
  corespernode: 24
prometheus:
  max_job_name_chars: 15
  job_dispatch: sbatch
  stat: squeue -u $username
  run_command: mpiexec -n $cores
  batch_header: slurm-prometheus
  remote: prometheus.cyfronet.pl
  home_path_template: /net/people/$username
  runtime_path_template: /net/scratch/people/$username
  modules:
    all:
      - load apps/lammps
  temp_path_template: $work_path/tmp
  queue: plgrid
  python_build: python2.7
  corespernode: 24
  nodes: 4
cirrus:
  max_job_name_chars: 15
  job_dispatch: sbatch
  budget: ec190-kbronik
  sshpass: ''
  manual_sshpass: true
  stat: squeue -u $username
  run_command: srun -n $cores --kill-on-bad-exit
  batch_header: slurm-cirrus
  remote: login.cirrus.ac.uk
  home_path_template: /lustre/home/ec190/kbronik
  runtime_path_template: /lustre/home/ec190/kbronik
  modules:
    all:
      - load lammps
  temp_path_template: $work_path/tmp
  corespernode: 36
  cpuspertask: 1
  job_name: lammps_Example
  job_wall_time: '00:20:00'
  partition_name: standard
  qos_name: standard
supermuc2:
  max_job_name_chars: 15
  job_dispatch: llsubmit
  run_command: poe
  batch_header: ll-supermuc
  bac_ensemble_namd_script: bac-supermuc
  bac_ties_script: bac-ties-supermuc
  remote: hw.supermuc.lrz.de
  home_path_template: /home/hpc/$project/$username
  runtime_path_template: /gpfs/work/$project/$username
  modules:
    - load namd/2.10
  module_load_at_connect: false
  temp_path_template: $work_path/tmp
  queue: standard
  python_build: lib64/python2.6
  corespernode: 28
ohm:
  home_path_template: /home/$username
  remote: ohm.chem.ucl.ac.uk
  run_command: mpirun -np 1
Kathleen:
  home_path_template: /home/$username
  remote: kathleen-hpc-prod-10-313.gtm.ucl.ac.uk
  modules:
    all:
      - load python/3.7.4
      - load lammps/16Mar18/userintel/intel-2018
myriad:
  home_path_template: /home/$username
  remote: myriad.rc.ucl.ac.uk
  batch_header: myriad-header
  job_dispatch: qsub
bluejoule:
  job_dispatch: llsubmit
  run_command: 'runjob --env-all -p $corespernode -n $cores :'
  run_ensemble_command: runjob -n $cores_per_replica
  batch_header: ll
  remote: login.joule.hartree.stfc.ac.uk
  home_path_template: /gpfs/home/$project/$groupname/$username
  runtime_path_template: /gpfs/home/$project/$groupname/$username
  modules:
    - load namd/2.9
  queue: standard
  corespernode: 16
  pwd: '#'
  stat: llq -u $username
bluewonder2:
  job_dispatch: bsub <
  run_command: mpiexec.hydra -n $cores
  run_ensemble_command: mpiexec.hydra -n $cores_per_replica
  batch_header: lsf2
  bac_ensemble_namd_script: bac-wonder-namd
  bac_ensemble_nmode_script: bac-wonder2-nmode
  bac_ensemble_nm_remote_script: bac-wonder2-nm-remote
  remote: phase2.wonder.hartree.stfc.ac.uk
  home_path_template: /gpfs/stfc/local/$project/$groupname/$username
  runtime_path_template: /gpfs/stfc/local/$project/$groupname/$username
  modules:
    - load namd/2.9
  queue: standard
  corespernode: 24
  pwd: '#'
oppenheimer:
  remote: oppenheimer.chem.ucl.ac.uk
  run_command: /opt/openmpi/gfortran/1.4.3/bin/mpirun -np $cores
  batch_header: sge_oppenheimer
  no_hg: true
  job_dispatch: qsub
  python_build: lib/python2.6
QCG_PJ:
  virtual_env_path: $home_path/VirtualEnv
  app_repository: $home_path/App_repo
  job_name: QCG_PilotJob_installation
Eagle:
  stat: >-
    squeue -u $username --Format=jobid,state,account,timeleft,timeused,username
    --noheader
  job_dispatch: sbatch
  cancel_job_command: scancel $jobID
  job_info_command: squeue --jobs $jobID
  manual_ssh: false
  corespernode: 28
  cores: 8
  memory: 6000
  batch_header_install_app: eagle-app
  PJ_PYheader: eagle-PJ-py
  PJ_header: eagle-PJ-header
  PJ_size: '3'
  PJ_wall_time: '20:00'
  finishedJobTags: &ref_0
    - CANCELLED
    - COMPLETED
    - DEADLINE
    - FAILED
    - NODE_FAIL
    - OUT_OF_MEMORY
    - PREEMPTED
    - TIMEOUT
  unfinishedJobTags: &ref_1
    - BOOT_FAIL
    - CONFIGURING
    - COMPLETING
    - PENDING
    - RUNNING
    - REQUEUED
    - STOPPED
    - SUSPENDED
qcg:
  virtual_env_path: $home_path/VirtualEnv
  app_repository: $home_path/App_repo
  job_name: QCG_PilotJob_installation
  remote: eagle.man.poznan.pl
  budget: vecma2020
  corespernode: 28
  cores: 1
  queue: plgrid
  memory: 6000
  batch_header_install_app: qcg-eagle-app
  PJ_PYheader: qcg-PJ-py
  PJ_header: qcg-PJ-header
  PJ_size: '4'
  job_wall_time: PT30M
  PJ_wall_time: PT3H
  home_path_template: /home/plgrid/$username
  manual_ssh: false
  dispatch_jobs_on_localhost: false
  run_command: mpirun -n $cores
  batch_header: qcg-eagle
  eagle_MODULEPATH: /home/plgrid-groups/plggvecma/.qcg-modules
  modules:
    loaded:
      - python/3.7.3
      - openmpi/4.0.0_gcc620
    unloaded:
      - python
  stat: qcg-list -Q -F "%-22I %-16S %-8H"
  job_dispatch: qcg-sub
  cancel_job_command: qcg-cancel $jobID
  finishedJobTags:
    - FINISHED
    - FAILED
    - CANCELED
  unfinishedJobTags:
    - UNSUBMITED
    - UNCOMMITED
    - QUEUED
    - PREPROCESSING
    - PENDING
    - RUNNING
    - STOPPED
    - POSTPROCESSING
eagle_vecma:
  stat: >-
    squeue -u $username --Format=jobid,state,account,timeleft,timeused,username
    --noheader
  job_dispatch: sbatch
  cancel_job_command: scancel $jobID
  job_info_command: squeue --jobs $jobID
  manual_ssh: false
  corespernode: 28
  cores: 8
  memory: 6000
  batch_header_install_app: eagle-app
  PJ_PYheader: eagle-PJ-py
  PJ_header: eagle-PJ-header
  PJ_size: '3'
  PJ_wall_time: '20:00'
  finishedJobTags: *ref_0
  unfinishedJobTags: *ref_1
  virtual_env_path: $home_path/VirtualEnv
  app_repository: $home_path/App_repo
  job_name: QCG_PilotJob_installation
  run_command: mpirun -n $cores
  remote: eagle.man.poznan.pl
  home_path_template: /home/plgrid/$username
  batch_header: slurm-eagle
  budget: plgvecma2021
  partition_name: altair
  job_wall_time: '0-0:30:00'
  modules:
    loaded:
      - python/3.7.3
      - openmpi/4.0.0_gcc620
    unloaded:
      - python
eagle_hidalgo:
  stat: >-
    squeue -u $username --Format=jobid,state,account,timeleft,timeused,username
    --noheader
  job_dispatch: sbatch
  cancel_job_command: scancel $jobID
  job_info_command: squeue --jobs $jobID
  manual_ssh: false
  corespernode: 28
  cores: 8
  memory: 6000
  batch_header_install_app: eagle-app
  PJ_PYheader: eagle-PJ-py
  PJ_header: eagle-PJ-header
  PJ_size: '4'
  PJ_wall_time: '0-3:00:00'
  finishedJobTags: *ref_0
  unfinishedJobTags: *ref_1
  virtual_env_path: $home_path/VirtualEnv
  app_repository: $home_path/App_repo
  job_name: QCG_PilotJob_installation
  run_command: mpirun -n $cores
  remote: eagle.man.poznan.pl
  home_path_template: /home/users/$username
  batch_header: slurm-eagle
  budget: '394'
  partition_name: altair
  job_wall_time: '0-0:30:00'
  modules:
    loaded:
      - python/3.7.3
      - openmpi/4.0.0_gcc620
    unloaded:
      - python
eagle_seavea:
  stat: >-
    squeue -u $username --Format=jobid,state,account,timeleft,timeused,username
    --noheader
  job_dispatch: sbatch
  cancel_job_command: scancel $jobID
  job_info_command: squeue --jobs $jobID
  manual_ssh: false
  corespernode: 28
  cores: 8
  memory: 6000
  batch_header_install_app: eagle-app
  PJ_PYheader: eagle-PJ-py
  PJ_header: eagle-PJ-header
  PJ_size: '4'
  PJ_wall_time: '0-3:00:00'
  finishedJobTags: *ref_0
  unfinishedJobTags: *ref_1
  virtual_env_path: $home_path/VirtualEnv
  app_repository: $home_path/App_repo
  job_name: QCG_PilotJob_installation
  run_command: mpirun -n $cores
  remote: eagle.man.poznan.pl
  home_path_template: /home/users/$username
  batch_header: slurm-eagle
  budget: '574'
  partition_name: standard
  job_wall_time: '0-0:30:00'
  modules:
    loaded:
      - python/3.7.3
      - openmpi/4.0.0_gcc620
    unloaded:
      - python
supermuc_vecma:
  stat: >-
    squeue -u $username --Format=jobid,state,account,timeleft,timeused,username
    --noheader
  job_dispatch: sbatch
  cancel_job_command: scancel $jobID
  job_info_command: squeue --jobs $jobID
  manual_ssh: false
  corespernode: 28
  cores: 8
  memory: 6000
  batch_header_install_app: supermuc-app
  PJ_PYheader: eagle-PJ-py
  PJ_header: supermuc-PJ-header
  PJ_size: '3'
  PJ_wall_time: '20:00'
  finishedJobTags: *ref_0
  unfinishedJobTags: *ref_1
  virtual_env_path: $home_path/VirtualEnv
  app_repository: $home_path/App_repo
  job_name: QCG_PilotJob_installation
  run_command: mpirun -n $cores
  remote: skx.supermuc.lrz.de
  home_path_template: /dss/dsshome1/09/$username
  batch_header: slurm-supermuc
  budget: pn72wa
  partition_name: general
  modules:
    loaded:
      - python/3.6_intel
    unloaded:
      - python
genji:
  home_path_template: /home_nfs_robin_ib/$username/$project
  runtime_path_template: /home_nfs_robin_ib/$username/$project
  batch_header: slurm-genji
  job_name: VECMA
  run_command: mpiexec.hydra -n $cores
  stat: squeue
  job_dispatch: sbatch
  PJ_header: slurm-genji-PJ-header
  PJ_PYheader: slurm-genji-PJ-py
  PJ_size: '3'
training_hidalgo:
  stat: >-
    squeue -u $username --Format=jobid,state,account,timeleft,timeused,username
    --noheader
  job_dispatch: sbatch
  cancel_job_command: scancel $jobID
  job_info_command: squeue --jobs $jobID
  manual_ssh: false
  corespernode: 28
  cores: 8
  memory: 6000
  batch_header_install_app: eagle-app
  PJ_PYheader: eagle-PJ-py
  PJ_header: eagle-PJ-header
  PJ_size: '4'
  PJ_wall_time: '0-3:00:00'
  finishedJobTags: *ref_0
  unfinishedJobTags: *ref_1
  virtual_env_path: $home_path/VirtualEnv
  app_repository: $home_path/App_repo
  job_name: QCG_PilotJob_installation
  run_command: mpirun -n $cores
  remote: 62.3.171.192
  home_path_template: /home/users/$username
  batch_header: slurm-eagle
  budget: '394'
  partition_name: standard
  job_wall_time: '0-0:30:00'
  run_prefix_commands:
    - export PYTHONPATH=/home/users/$username/.local/lib/python3.7/site-packages
  modules:
    loaded:
      - python/3.7.3
      - openmpi/1.8.1-1_gcc482
    unloaded:
      - python
archer2:
  virtual_env_path: /work/$project/$project/$username/VirtualEnv
  app_repository: $home_path/App_repo
  remote: archer2
  budget: e723-brunel
  project: e723
  corespernode: 128
  run_command: 'srun --distribution=block:block --hint=nomultithread'
  qos_name: standard
  partition_name: standard
  batch_header: slurm-archer2
  PJ_header: archer2-PJ-header
  batch_header_install_app: archer-app
  home_path_template: /work/$project/$project/$username
  modules:
    all:
      - load cray-python
  run_prefix_commands:
    - export PYTHONUSERBASE=/work/$project/$project/$username/.local
    - 'export PATH=$PYTHONUSERBASE/bin:$PATH'
    - 'export PYTHONPATH=$PYTHONUSERBASE/lib/python3.8/site-packages:$PYTHONPATH'
    - export SLURM_CPU_FREQ_REQ=2250000
    - export ENV_DIR=/work/$project/$project/$username/VirtualEnv
marenostrum4:
  stat: >-
    squeue -u $username --Format=jobid,state,account,timeleft,timeused,username
    --noheader
  job_dispatch: sbatch
  cancel_job_command: scancel $jobID
  job_info_command: squeue --jobs $jobID
  manual_ssh: false
  corespernode: 28
  cores: 8
  memory: 6000
  batch_header_install_app: eagle-app
  PJ_PYheader: eagle-PJ-py
  PJ_header: eagle-PJ-header
  PJ_size: '3'
  PJ_wall_time: '20:00'
  finishedJobTags: *ref_0
  unfinishedJobTags: *ref_1
  virtual_env_path: $home_path/VirtualEnv
  app_repository: $home_path/App_repo
  job_name: QCG_PilotJob_installation
  remote: mn1.bsc.es
  batch_header: slurm-marenostrum4
  job_wall_time: '0-0:10:00'
  modules:
    loaded:
      - python/3.8.2
    unloaded:
      - python
piz_daint:
  stat: >-
    squeue -u $username --Format=jobid,state,account,timeleft,timeused,username
    --noheader
  job_dispatch: sbatch
  cancel_job_command: scancel $jobID
  job_info_command: squeue --jobs $jobID
  manual_ssh: false
  corespernode: 28
  cores: 8
  memory: 6000
  batch_header_install_app: eagle-app
  PJ_PYheader: eagle-PJ-py
  PJ_header: eagle-PJ-header
  PJ_size: '3'
  PJ_wall_time: '20:00'
  finishedJobTags: *ref_0
  unfinishedJobTags: *ref_1
  virtual_env_path: $home_path/VirtualEnv
  app_repository: $home_path/App_repo
  job_name: QCG_PilotJob_installation
  remote: daint.cscs.ch
monsoonfab:
  remote: “monsoonfab”
hsuper:
  remote: "hsuper-login01.hsu-hh.de"
  cores: 1
  corespernode: 72
  partition_name: "small"
  batch_header: "slurm-hsuper"
  qos_name: "normal"
  job_dispatch: sbatch
  cancel_job_command: "scancel $jobID"
  job_wall_time: "0-0:10:00"
  job_info_command: "squeue --jobs $jobID"
  stat: >-
    squeue -u $username --Format=JobID:.10,Partition:.11,Name:.70,UserName:.10,State:.8,TimeUsed:.13,TimeLimit:.9,NumNodes:.6,Reason
    --noheader
