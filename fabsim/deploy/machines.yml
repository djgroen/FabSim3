default:
  # General default values for all machines live in the default dictionary.
  # Machine-specific values override (replace) these.
  # User-specific values in machines_user.yml override machine-specific values.

  # Default Modules to load or unload on remote, grouped by scriptname (or 'all' if to be used by all scripts).
  # e.g. ['load cmake'].
  modules: 
    all: []
    dummy: []

  # Number of process when launch "run_ensemble"command
  nb_process: 1

  # loading virtual environment on the remote machine 
  venv : False
  # Allows to load or unload default modules on remote via interactive connection
  module_load_at_connect: true
  
  # Default number of cores per job.
  cores: 16
  
  # Commands to execute remotely as part of remote run jobs, prior to execution of the main code.
  run_prefix_commands: [". /home/mghorbani/workspace/python/venv/bin/activate"]
  
  # Templates to use to find space to build codes and execute workflows remotely.
  # All user- and generally- defined values in these config dictionaries will be interpolated via $foo
  home_path_template: "/home/$username"
  
  # Name for the Project FabSim3 folder
  fabric_dir: "FabSim"
  
  # Path to runtime accessible filesystem (default is same as build time) - OBSOLETE?
  runtime_path_template: "$home_path"
  
  # Default number of cores per (supercomputer) node.
  corespernode: 1

  # Default number of cpus per task
  cpuspertask: 1

  # Default number of nodes
  nodes: 1
  
  # Default number of cores per replica in an ensemble. - OBSOLETE?
  cores_per_replica: "${cores}"
  
  # Default command to use to launch remote jobs
  # e.g. qsub, empty string to launch interactively (e.g. on localhost, a VM or a machine without scheduler).
  job_dispatch: ''
  
  # Path to build filesystem folder
  # All user-defined values in these config dictionaries will be interpolated via $foo
  remote_path_template: "$home_path/$fabric_dir"
  
  # Path to run filesystem folder
  work_path_template: "$runtime_path/$fabric_dir"
  
  # Default batch job header template.
  # These templates are located in the templates/ subdirectory.
  batch_header: no_batch
  
  # Default command used to launch jobs on the nodes of a specific machine.
  run_command: "mpirun -np $cores"
  
  # Default location of the temporary file path - OBSOLETE?
  temp_path_template: "/tmp"
  
  # Default naming scheme used to label FabSim3 jobs.
  job_desc: ""  
  job_name_template: '${config}_${machine_name}_${cores}${job_desc}'

  # Flag to bypass paramiko and do direct remote SSH commands.
  manual_ssh: false
  
  # Flag to use GSISSH instead of SSH.
  manual_gsissh: false
  
  # Flag to use sshpass within SSH (requires that either value for sshpass is set with filename of plaintext password file for this machine or SSHPASS set in environment)
  manual_sshpass: false

  # Flag to use 'monsoon mode', which avoids rsync altogether and copies 
  # using recursive scp commands.
  ssh_monsoon_mode: false

  # Default location for templates.
  local_templates_path: ["$fabsim_root/deploy/templates"]

  # Default command syntax for getting the job status.
  stat: "qstat -u $username"
  
  # Default command for cancelling jobs.
  cancel_job_command: "qdel"
  
  # Default location for config_files.
  local_config_file_path: ["$fabsim_root/config_files"]

  # Default command for Python3
  python3_command: "python3"

  # possible inputs are: ["delete","ignore","warn_only"]
  # 1- delete : delte the exist results folder 
  # 2- ignore : ignore the exist results folder 
  # 2- warn_only : warn user and keep the exist results folder
  prevent_results_overwrite: "ignore"

  # enable rich Console for colorful outputs
  rich_console: True

  # Dry Run shows the job script generated by FabSim3 without
  # submitting it to the remote machine
  dry_run: False

cartesius:
  max_job_name_chars: 15
  job_dispatch: "sbatch"
  stat: "squeue -u $username"
  run_command: "mpiexec -n $cores"
  batch_header: slurm-cartesius
  remote: "cartesius.surfsara.nl"
  home_path_template: "/home/$username"
  runtime_path_template: "/scratch-local/$username"
  modules:
    all: []
    gromacs: ["load gromacs"]
  temp_path_template: "$work_path/tmp"
  queue: "normal"
  python_build: "python2.7"
  corespernode: 24

prometheus:
  max_job_name_chars: 15
  job_dispatch: "sbatch"
  stat: "squeue -u $username"
  run_command: "mpiexec -n $cores"
  batch_header: slurm-prometheus
  remote: "prometheus.cyfronet.pl"
  home_path_template: "/net/people/$username"
  runtime_path_template: "/net/scratch/people/$username"
  modules: 
    all: ["load apps/lammps"]
  temp_path_template: "$work_path/tmp"
  queue: "plgrid"
  python_build: "python2.7"
  corespernode: 24
  nodes: 4

cirrus:
  max_job_name_chars: 15
  job_dispatch: "sbatch"
  budget: "ec190-kbronik"
  sshpass: ''
  manual_sshpass: true
  stat: "squeue -u $username"
  run_command: "srun -n $cores --kill-on-bad-exit"
  batch_header: slurm-cirrus
  remote: "login.cirrus.ac.uk"
  home_path_template: "/lustre/home/ec190/kbronik"
  runtime_path_template: "/lustre/home/ec190/kbronik"
  modules:
    all: ["load lammps"]
  temp_path_template: "$work_path/tmp"
  corespernode: 36
  cpuspertask: 1
  job_name: "lammps_Example"
  job_wall_time : "00:20:00"
  partition_name: "standard"
  qos_name: "standard"
#  password: 'NP1085B?(")")($&2021pass'



supermuc2:
  max_job_name_chars: 15
  job_dispatch: "llsubmit"
  run_command: "poe"
  batch_header: ll-supermuc
  bac_ensemble_namd_script: bac-supermuc
  bac_ties_script: bac-ties-supermuc
  remote: "hw.supermuc.lrz.de"
  home_path_template: "/home/hpc/$project/$username"
  runtime_path_template: "/gpfs/work/$project/$username"
  modules: ["load namd/2.10"] #-25Sep11"]
  module_load_at_connect: false
  temp_path_template: "$work_path/tmp"
  queue: "standard"
  python_build: "lib64/python2.6"
  corespernode: 28



ohm:
  home_path_template: "/home/$username"
  remote: "ohm.chem.ucl.ac.uk"
  run_command: "mpirun -np 1"


Kathleen:
  home_path_template: "/home/$username"
  remote: "kathleen-hpc-prod-10-313.gtm.ucl.ac.uk"
  modules:
    # list of modules to be loaded on remote machine
    all: ["load python/3.7.4", "load lammps/16Mar18/userintel/intel-2018"]

myriad:
  home_path_template: "/home/$username"
  remote: "myriad.rc.ucl.ac.uk"
  batch_header: myriad-header

bluejoule:
  job_dispatch: "llsubmit"
  run_command: "runjob --env-all -p $corespernode -n $cores :"
  run_ensemble_command: "runjob -n $cores_per_replica"
  batch_header: ll
  remote: "login.joule.hartree.stfc.ac.uk"
  home_path_template: "/gpfs/home/$project/$groupname/$username"
  runtime_path_template: "/gpfs/home/$project/$groupname/$username"
  modules: ["load namd/2.9"]
  queue: "standard"
  corespernode: 16
  pwd: "#"
  stat: "llq -u $username"
bluewonder2:
  job_dispatch: "bsub <"
  run_command: "mpiexec.hydra -n $cores"
  run_ensemble_command: "mpiexec.hydra -n $cores_per_replica"
  batch_header: lsf2
  bac_ensemble_namd_script: bac-wonder-namd
  bac_ensemble_nmode_script: bac-wonder2-nmode
#  bac_ensemble_nmode_script: bac-wonder2-nmode-test
  bac_ensemble_nm_remote_script: bac-wonder2-nm-remote
  remote: "phase2.wonder.hartree.stfc.ac.uk"
  home_path_template: "/gpfs/stfc/local/$project/$groupname/$username"
  runtime_path_template: "/gpfs/stfc/local/$project/$groupname/$username"
  modules: ["load namd/2.9"]
  queue: "standard"
  corespernode: 24
  pwd: "#"
oppenheimer:
  remote: "oppenheimer.chem.ucl.ac.uk"
  run_command: "/opt/openmpi/gfortran/1.4.3/bin/mpirun -np $cores"
  batch_header: sge_oppenheimer
  no_hg: true
  job_dispatch: "qsub"
  python_build: "lib/python2.6"
localhost:
  remote: "localhost"
  python_build: "lib/python2.7"
  home_path_template: "/home/$username"
  manual_ssh: true
  batch_header: bash_header
  # The setting below can be useful for debugging if you run into problems.
  # manual_ssh: true

QCG_PJ: &QCG_PilotJob
  # pilot job manager installation  
  virtual_env_path: "$home_path/VirtualEnv"
  app_repository: "$home_path/App_repo"

  #virtual_env_path: "$home_path/VirtualEnv3.7"
  #app_repository: "$home_path/App_repo3.7"

  job_name: "QCG_PilotJob_installation" 


Eagle : &Eagle_Default_Config
  # stat: "sacct -o jobid,state,cluster -X -n"
  # stat: "sacct --format=jobid,state,cluster --allocations --noheader"
  stat: "squeue -u $username --Format=jobid,state,account,timeleft,timeused,username --noheader"
  job_dispatch: "sbatch"
  cancel_job_command: 'scancel $jobID'
  # job_info_command : 'sacct -X -j $jobID'  
  # job_info_command : 'sacct --allocations --job $jobID'
  job_info_command : 'squeue --jobs $jobID'
  
  manual_ssh: false
  corespernode: 28
  cores: 8
  memory: 6000

  # template header for VirtualEnv
  batch_header_install_app : eagle-app

  # PilotJob attributes  
  PJ_PYheader: eagle-PJ-py
  PJ_header: eagle-PJ-header
  #PJ_header: no_batch
  PJ_size : "3"
  PJ_wall_time : "20:00"

  # ref : https://slurm.schedmd.com/squeue.html#lbAG
  finishedJobTags: ["CANCELLED", "COMPLETED", "DEADLINE", "FAILED", "NODE_FAIL", "OUT_OF_MEMORY", "PREEMPTED", "TIMEOUT"   ]
  unfinishedJobTags: ["BOOT_FAIL", "CONFIGURING", "COMPLETING", "PENDING", "RUNNING", "REQUEUED", "STOPPED", "SUSPENDED"]


qcg:
  <<: *QCG_PilotJob
  remote: "eagle.man.poznan.pl"   
  budget: "vecma2020"
  corespernode: 28
  cores: 1
  queue: "plgrid"
  memory: 6000
  # template header for VirtualEnv
  batch_header_install_app : qcg-eagle-app
  # PilotJob header template
  
  # PilotJob attributes  
  PJ_PYheader: qcg-PJ-py
  PJ_header: qcg-PJ-header
  #PJ_header: no_batch
  PJ_size : "4"
  # qcg walltime : P[nY][nM][nD][T[nH][nM]nS]  : P3DT12H
  job_wall_time : "PT30M"
  PJ_wall_time : "PT3H"

  home_path_template: "/home/plgrid/$username"
  manual_ssh: false
  dispatch_jobs_on_localhost: false #TODO: implement this in fab/base in the def job() function, so that job dispatch is done locally.
  run_command: "mpirun -n $cores"

  batch_header: qcg-eagle

  
  #run_prefix_commands: ["source $virtual_env_path/bin/activate"]

  eagle_MODULEPATH : "/home/plgrid-groups/plggvecma/.qcg-modules"

  modules:
    # list of modules to be loaded on remote machine
    loaded: ["python/3.7.3", "openmpi/4.0.0_gcc620"]
    # list of modules to be unloaded on remote machine
    unloaded: ["python"]




  # stat : return report for all submitted jobs
  # it should be set in the way that only return jobID, job status, and host in the output (without column header)
  # for example: the template output should be similar to :
  #           ...   ...       ...
  #           job1  FINISHED  eagle
  #           job2  FAILED    eagle
  #           job2  CANCELED  eagle
  #           ...   ...       ...  
  # stat: 'qcg-list -Q -s all -F "%-22I %-16S %-8H" | awk "{if(NR>2)print}"'
  stat: 'qcg-list -Q -F "%-22I %-16S %-8H"'

  # job_dispatch : the command to submit the task to be processed by remote machine
  job_dispatch: "qcg-sub"

  # cancel_job_command : cancel submitted task based on input $jobID 
  # the $jobID variable will be set during the execution, keep the format as '$jobID' in the parameters list
  cancel_job_command: 'qcg-cancel $jobID'

  # ref : http://apps.man.poznan.pl/trac/qcg-broker/wiki/qcg-tools#qcg-list
  finishedJobTags: ["FINISHED","FAILED", "CANCELED"]
  unfinishedJobTags: ["UNSUBMITED", "UNCOMMITED", "QUEUED", "PREPROCESSING", "PENDING", "RUNNING", "STOPPED", "POSTPROCESSING"]

eagle_vecma:
  <<: *Eagle_Default_Config
  <<: *QCG_PilotJob
  run_command: "mpirun -n $cores"
  remote: "eagle.man.poznan.pl"
  home_path_template: "/home/plgrid/$username"
  batch_header: slurm-eagle
  budget: "plgvecma2021"
  # list of available partitions : sinfo --Format=partition
  partition_name: "altair"

  # wall time in format days-hours:minutes:seconds
  job_wall_time : "0-0:30:00"

  modules:
    # list of modules to be loaded on remote machine
    loaded: ["python/3.7.3", "openmpi/4.0.0_gcc620"]
    # list of modules to be unloaded on remote machine
    unloaded: ["python"]
  
eagle_hidalgo:
  <<: *Eagle_Default_Config
  <<: *QCG_PilotJob
  run_command: "mpirun -n $cores"
  remote: "eagle.man.poznan.pl"
  home_path_template: "/home/users/$username"
  batch_header: slurm-eagle
  budget: "394"
  # list of available partitions : sinfo --Format=partition
  partition_name: "altair"
  # wall time in format days-hours:minutes:seconds
  job_wall_time : "0-0:30:00"

  PJ_size : "4"
  PJ_wall_time : "0-3:00:00"


  modules:
    # list of modules to be loaded on remote machine
    loaded: ["python/3.7.3", "openmpi/4.0.0_gcc620"]
    # list of modules to be unloaded on remote machine
    unloaded: ["python"]

eagle_seavea:
  <<: *Eagle_Default_Config
  <<: *QCG_PilotJob
  run_command: "mpirun -n $cores"
  remote: "eagle.man.poznan.pl"
  home_path_template: "/home/users/$username"
  batch_header: slurm-eagle
  budget: "574"
  # list of available partitions : sinfo --Format=partition
  partition_name: "standard"
  # wall time in format days-hours:minutes:seconds
  job_wall_time : "0-0:30:00"

  PJ_size : "4"
  PJ_wall_time : "0-3:00:00"


  modules:
    # list of modules to be loaded on remote machine
    loaded: ["python/3.7.3", "openmpi/4.0.0_gcc620"]
    # list of modules to be unloaded on remote machine
    unloaded: ["python"]

supermuc_vecma:
  <<: *Eagle_Default_Config
  <<: *QCG_PilotJob
  run_command: "mpirun -n $cores"
  remote: "skx.supermuc.lrz.de"
  home_path_template: "/dss/dsshome1/09/$username"
  batch_header: slurm-supermuc
  budget: "pn72wa"

  batch_header_install_app : supermuc-app
  PJ_header: supermuc-PJ-header

  # list of available partitions : sinfo --Format=partition
  partition_name: "general"

  modules:
    # list of modules to be loaded on remote machine
    loaded: ["python/3.6_intel"]
    # list of modules to be unloaded on remote machine
    unloaded: ["python"]

genji: 
  home_path_template: "/home_nfs_robin_ib/$username/$project"
  runtime_path_template: "/home_nfs_robin_ib/$username/$project"
  batch_header: slurm-genji
  job_name: "VECMA"
  run_command: "mpiexec.hydra -n $cores"
  stat: "squeue"
  job_dispatch: "sbatch"
  PJ_header: slurm-genji-PJ-header
  PJ_PYheader: slurm-genji-PJ-py
  PJ_size: "3"

training_hidalgo:
  <<: *Eagle_Default_Config
  <<: *QCG_PilotJob
  run_command: "mpirun -n $cores"
  remote: "62.3.171.192"
  home_path_template: "/home/users/$username"
  batch_header: slurm-eagle
  budget: "394"
  # list of available partitions : sinfo --Format=partition
  partition_name: "standard"
  # wall time in format days-hours:minutes:seconds
  job_wall_time : "0-0:30:00"

  PJ_size : "4"
  PJ_wall_time : "0-3:00:00"

  run_prefix_commands: ["export PYTHONPATH=/home/users/$username/.local/lib/python3.7/site-packages"]

  modules:
    # list of modules to be loaded on remote machine
    #loaded: ["python/3.7.3", "openmpi/4.0.0_gcc620"]
    loaded: ["python/3.7.3", "openmpi/1.8.1-1_gcc482"]
    
    # list of modules to be unloaded on remote machine
    unloaded: ["python"]

archer2:
  <<: *Eagle_Default_Config
  <<: *QCG_PilotJob  
  remote: "archer2" #ARCHER2 always requires multiplexing, so here we need an alias.
  budget: "d137"
  project: "d137"
  corespernode: 128
  run_command: "srun -n $cores"
  qos_name: "standard"
  # list of available partitions : sinfo --Format=partition
  job_dispatch: "cd /work/$project/$project/$username ; sbatch"
  partition_name: "standard"
  batch_header: slurm-archer2
  PJ_header: archer2-PJ-header
  batch_header_install_app : archer-app
  # all required files at runtime, must be on the /work filesystem, 
  # in ARCHER2, there is not access to /home on compute side
  home_path_template: "/work/$project/$project/$username"   
  modules:
    all: ["load cray-python"]
  run_prefix_commands: ["export PYTHONUSERBASE=/work/$project/$project/$username/.local", "export PATH=$PYTHONUSERBASE/bin:$PATH", "export PYTHONPATH=$PYTHONUSERBASE/lib/python3.8/site-packages:$PYTHONPATH", "export SLURM_CPU_FREQ_REQ=2250000"]

marenostrum4:
  <<: *Eagle_Default_Config
  <<: *QCG_PilotJob

  remote: "mn1.bsc.es"
  batch_header: slurm-marenostrum4

  # wall time in format days-hours:minutes:seconds
  job_wall_time : "0-0:10:00"

  modules:
    # list of modules to be loaded on remote machine
    loaded: ["python/3.8.2"]
    # list of modules to be unloaded on remote machine
    unloaded: ["python"]


piz_daint:
  <<: *Eagle_Default_Config
  <<: *QCG_PilotJob
  remote: "daint.cscs.ch"

monsoonfab:
  remote: “monsoonfab” #Monsoon always requires multiplexing due to the RSA fob, so here we need an alias.
